### Gradient Descent Algorithm

#### Outline

- Start with some theta0, theta1 (eg. 0,0)
- Keep changing thetas (each time take the best step), hopefully we are able to minimize cost function J(t0,t1)

- simultaneous update of gradient descent
